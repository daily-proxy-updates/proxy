import json
import os
import random
import datetime
try:
    from urllib.parse import urlparse
except ImportError:
    from urlparse import urlparse # Python 2 fallback

# é…ç½®éƒ¨åˆ†
SITES_DIR = '../mirror/sites'  # ç›¸å¯¹è·¯å¾„ï¼ŒæŒ‡å‘ mirror é¡¹ç›®çš„ sites ç›®å½•
OUTPUT_FILE = 'README.md'
# å…³é”®è¯åˆ—è¡¨ï¼Œç”¨äºéšæœºæ’å…¥æˆ–ä½œä¸ºæ ‡é¢˜
KEYWORDS = [
    "æœºåœºæ¨è", "ç§‘å­¦ä¸Šç½‘", "ç¿»å¢™æ•™ç¨‹", "VPNæ¨è", "ClashèŠ‚ç‚¹", 
    "ShadowsocksèŠ‚ç‚¹", "V2RayèŠ‚ç‚¹", "TrojanèŠ‚ç‚¹", "å…è´¹èŠ‚ç‚¹", 
    "é«˜é€Ÿæ¢¯å­", "ç¨³å®šæœºåœº", "æµåª’ä½“è§£é”", "ChatGPTè§£é”", 
    "4Kç§’å¼€", "æ™šé«˜å³°ä¸å¡", "IPLCä¸“çº¿", "ä¸­è½¬æœºåœº"
]

def load_sites_and_links():
    """è¯»å–æ‰€æœ‰ç«™ç‚¹é…ç½®å’Œå¯¹åº”çš„é“¾æ¥æ–‡ä»¶"""
    sites_data = []
    
    if not os.path.exists(SITES_DIR):
        print(f"Error: Sites directory {SITES_DIR} not found.")
        return []

    # è·å–æ‰€æœ‰ç«™ç‚¹é…ç½®æ–‡ä»¶
    site_files = [f for f in os.listdir(SITES_DIR) if f.endswith('.json') and not f.endswith('_links.json')]
    
    for site_file in site_files:
        site_id = site_file.replace('.json', '')
        site_path = os.path.join(SITES_DIR, site_file)
        links_path = os.path.join(SITES_DIR, f"{site_id}_links.json")
        
        try:
            with open(site_path, 'r', encoding='utf-8') as f:
                site_config = json.load(f)
            
            # åªæœ‰å½“å¯¹åº”çš„ links æ–‡ä»¶å­˜åœ¨æ—¶æ‰å¤„ç†
            if os.path.exists(links_path):
                with open(links_path, 'r', encoding='utf-8') as f:
                    links = json.load(f)
                
                valid_links = []
                for kw, url in links.items():
                    if url.startswith('http'):
                         # å°è¯•ä½¿ç”¨é•œåƒåŸŸåæ›¿æ¢åŸå§‹åŸŸå
                        proxy_host = site_config.get('proxyHost')
                        final_url = url
                        if proxy_host:
                            try:
                                # è§£æåŸå§‹ URL
                                parsed_url = urlparse(url)
                                # æ›¿æ¢ host ä¸ºé•œåƒ host
                                # å¦‚æœ proxy_host æ˜¯æ•°ç»„ï¼Œå–ç¬¬ä¸€ä¸ª
                                host_to_use = proxy_host[0] if isinstance(proxy_host, list) else proxy_host
                                final_url = url.replace(parsed_url.netloc, host_to_use)
                            except:
                                pass
                        
                        valid_links.append({'title': kw, 'url': final_url})
                
                if valid_links:
                    sites_data.append({
                        'name': site_config.get('name', site_id),
                        'proxy_host': site_config.get('proxyHost', ''),
                        'links': valid_links
                    })
        except Exception as e:
            print(f"Error processing {site_id}: {e}")
            
    return sites_data

def generate_content(sites_data, count=15):
    """ç”Ÿæˆ Markdown å†…å®¹"""
    all_articles = []
    
    # æ”¶é›†æ‰€æœ‰æ–‡ç« é“¾æ¥
    for site in sites_data:
        for link in site['links']:
            all_articles.append({
                'title': link['title'],
                'url': link['url'],
                'site_name': site['name']
            })
    
    if not all_articles:
        return "No articles found."

    # éšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„æ–‡ç« 
    selected_articles = random.sample(all_articles, min(count, len(all_articles)))
    
    # ç”Ÿæˆ Markdown
    today = datetime.date.today().strftime("%Y-%m-%d")
    md_content = f"# æœºåœºæ¨èä¸ç½‘ç»œåŠ é€ŸæŒ‡å— ({today})\n\n"
    
    md_content += "> æœ¬æ–‡æ•´ç†äº†æœ€æ–°çš„ç½‘ç»œåŠ é€Ÿèµ„æºä¸æŠ€å·§ï¼ŒåŠ©ä½ ç•…æ¸¸äº’è”ç½‘ã€‚\n\n"
    
    # éšæœºæ’å…¥ä¸€äº›å…³é”®è¯æ®µè½
    md_content += f"**çƒ­é—¨æ ‡ç­¾**ï¼š{'ã€'.join(random.sample(KEYWORDS, 5))}\n\n"
    
    md_content += "## ç²¾é€‰æ–‡ç« \n\n"
    
    for article in selected_articles:
        # éšæœºç»™æ ‡é¢˜åŠ ä¸€äº› emoji
        emoji = random.choice(["ğŸš€", "âš¡", "ğŸŒ", "ğŸ”¥", "ğŸ’¡", "ğŸ“"])
        md_content += f"### {emoji} [{article['title']}]({article['url']})\n\n"
        # å¯ä»¥åœ¨è¿™é‡ŒåŠ ä¸€äº›éšæœºç”Ÿæˆçš„æè¿°æ–‡æœ¬ï¼Œå¢åŠ  SEO
        md_content += f"äº†è§£æ›´å¤šå…³äº {article['title']} çš„è¯¦ç»†å†…å®¹ï¼Œè¯·ç‚¹å‡»ä¸Šæ–¹é“¾æ¥è®¿é—®ã€‚\n\n"
        
    md_content += "---\n"
    md_content += f"*è‡ªåŠ¨æ›´æ–°äº {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"
    
    return md_content

def main():
    print("å¼€å§‹ç”Ÿæˆå†…å®¹...")
    sites = load_sites_and_links()
    content = generate_content(sites, count=random.randint(10, 20))
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"å†…å®¹å·²ç”Ÿæˆè‡³ {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
