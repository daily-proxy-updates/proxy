import os
import json
import random
import datetime
import requests
import xml.etree.ElementTree as ET
from urllib.parse import urlparse, quote

# é…ç½®
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
SITES_DIR = os.path.join(PROJECT_ROOT, 'sites')
OUTPUT_FILE = os.path.join(PROJECT_ROOT, 'README.md')

# å…³é”®è¯åº“
KEYWORDS = [
    "æœºåœºæ¨è", "ç§‘å­¦ä¸Šç½‘", "æ¢¯å­æ¨è", "ç¿»å¢™è½¯ä»¶", "VPNæ¨è", 
    "ClashèŠ‚ç‚¹", "Shadowsocks", "V2Ray", "Trojan", "é«˜é€ŸèŠ‚ç‚¹",
    "è§£é”Netflix", "4Kç§’å¼€", "ç¨³å®šæœºåœº", "ä¾¿å®œæœºåœº", "IPLCä¸“çº¿"
]

# æ ‡é¢˜æ¨¡æ¿
TITLE_TEMPLATES = [
    "{name} æœºåœºæ¨è - é«˜é€Ÿç¨³å®š 4K ç§’å¼€",
    "2024 æœ€ä½³æœºåœºæ¨èï¼š{name} è¯„æµ‹",
    "{name} æ€ä¹ˆæ ·ï¼Ÿæœ€æ–°ä½¿ç”¨ä½“éªŒæŠ¥å‘Š",
    "ç¨³å®šå¥½ç”¨çš„æ¢¯å­æ¨èï¼š{name}",
    "{name} - è§£é”æµåª’ä½“ï¼Œæ™šé«˜å³°ä¸å¡é¡¿",
    "ä¾¿å®œæœºåœºæ¨èï¼š{name} æ€§ä»·æ¯”ä¹‹é€‰",
    "{name} å®˜ç½‘åœ°å€ä¸æœ€æ–°ä¼˜æƒ ç ",
    "å®‰å“/iOS/Mac/Windows é€šç”¨æœºåœºæ¨èï¼š{name}"
]

def fetch_feed_posts(proxy_host):
    """Fetch posts from the site's RSS feed"""
    # Try different schemes if https fails, but default to https
    url = f"https://{proxy_host}/feed"
    print(f"Fetching feed from: {url}")
    try:
        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        if response.status_code == 200:
            # Parse XML
            try:
                content = response.content
                try:
                    text = content.decode('utf-8')
                except UnicodeDecodeError:
                    text = content.decode('gbk', errors='ignore')
                root = ET.fromstring(text)
            except ET.ParseError as e:
                print(f"XML Parse Error for {proxy_host}: {e}")
                return []
                
            posts = []
            # Standard RSS 2.0: channel -> item
            # Also handle Atom if needed, but WordPress uses RSS 2.0 by default at /feed
            for item in root.findall('./channel/item'):
                title = item.find('title')
                link = item.find('link')
                
                if title is not None and link is not None:
                    title_text = title.text
                    link_text = link.text
                    
                    # Ensure link uses proxy_host
                    if link_text:
                        parsed = urlparse(link_text)
                        # Reconstruct url with proxy_host
                        new_link = link_text.replace(parsed.netloc, proxy_host)
                        
                        posts.append({
                            'name': title_text, # Use title as name
                            'url': new_link,
                            'type': 'article'
                        })
            print(f"Found {len(posts)} posts for {proxy_host}")
            return posts
        else:
            print(f"Failed to fetch feed for {proxy_host}: Status {response.status_code}")
    except Exception as e:
        print(f"Error fetching feed for {proxy_host}: {e}")
    return []

def get_all_site_configs():
    """è·å–æ‰€æœ‰ç«™ç‚¹é…ç½®"""
    configs = []
    if not os.path.exists(SITES_DIR):
        print(f"Error: Sites directory {SITES_DIR} not found.")
        return []

    site_files = [f for f in os.listdir(SITES_DIR) if f.endswith('.json') and not f.endswith('_links.json')]
    
    for site_file in site_files:
        try:
            site_path = os.path.join(SITES_DIR, site_file)
            with open(site_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                # Ensure we have the filename id
                config['id'] = site_file.replace('.json', '')
                configs.append(config)
        except Exception as e:
            print(f"Error loading config {site_file}: {e}")
            
    return configs

def fetch_site_data(site_config):
    """è·å–å•ä¸ªç«™ç‚¹çš„æ•°æ®ï¼ˆæ–‡ç« å’Œé“¾æ¥ï¼‰"""
    site_id = site_config.get('id')
    proxy_host = site_config.get('proxyHost')
    
    if not proxy_host:
        return []
        
    if isinstance(proxy_host, list):
        proxy_host = proxy_host[0]
        
    valid_links = []
    
    # 1. è·å– RSS æ–‡ç«  (Sub-pages)
    feed_posts = fetch_feed_posts(proxy_host)
    if feed_posts:
        valid_links.extend(feed_posts)
        
    # 2. è¯»å–å¹¶è½¬æ¢æ¨å¹¿é“¾æ¥ (Redirects)
    links_path = os.path.join(SITES_DIR, f"{site_id}_links.json")
    if os.path.exists(links_path):
        try:
            with open(links_path, 'r', encoding='utf-8') as f:
                links = json.load(f)
            
            for kw, url in links.items():
                if url.startswith('http'):
                    # Convert to local redirect link: https://site.com/keyword
                    # Server handles this via dynamicLinks
                    # We assume server uses decoded path matching, so we can use encoded or decoded.
                    # Standard URL should be encoded.
                    encoded_kw = quote(kw)
                    local_url = f"https://{proxy_host}/{encoded_kw}"
                    
                    valid_links.append({
                        'name': kw,
                        'url': local_url,
                        'type': 'referral'
                    })
        except Exception as e:
            print(f"Error loading links for {site_id}: {e}")

    # 3. æ·»åŠ ç«™ç‚¹ä¸»é¡µ
    valid_links.append({
        'name': site_config.get('name', site_id),
        'url': f"https://{proxy_host}",
        'type': 'site'
    })
    
    return valid_links

def generate_title(item):
    """æ ¹æ®é“¾æ¥ç±»å‹ç”Ÿæˆæ ‡é¢˜"""
    name = item['name']
    
    # å¦‚æœæ˜¯æ–‡ç« ç±»å‹çš„é“¾æ¥ï¼Œç›´æ¥ä½¿ç”¨æ–‡ç« æ ‡é¢˜
    if item.get('type') == 'article':
        return name
        
    # é¦–å­—æ¯å¤§å†™
    name = name.capitalize() if name else "Unknown"
    
    template = random.choice(TITLE_TEMPLATES)
    return template.format(name=name)

def generate_content(all_items, count=15):
    """ç”Ÿæˆ Markdown å†…å®¹"""
    if not all_items:
        return "No articles found."

    # éšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„æ–‡ç« 
    # Ensure we don't error if count > len
    selected_items = random.sample(all_items, min(count, len(all_items)))
    
    # ç”Ÿæˆ Markdown
    today = datetime.date.today().strftime("%Y-%m-%d")
    md_content = f"# æœºåœºæ¨èä¸ç½‘ç»œåŠ é€ŸæŒ‡å— ({today})\n\n"
    
    md_content += "> æœ¬æ–‡æ¯æ—¥è‡ªåŠ¨æ›´æ–°ï¼Œæ•´ç†äº†æœ€æ–°çš„ç½‘ç»œåŠ é€Ÿèµ„æºã€æœºåœºæ¨èä¸ç§‘å­¦ä¸Šç½‘æŠ€å·§ï¼ŒåŠ©ä½ ç•…æ¸¸äº’è”ç½‘ã€‚\n\n"
    
    # éšæœºæ’å…¥ä¸€äº›å…³é”®è¯æ®µè½
    tags = random.sample(KEYWORDS, min(5, len(KEYWORDS)))
    md_content += f"**çƒ­é—¨æ ‡ç­¾**ï¼š{'ã€'.join(tags)}\n\n"
    
    md_content += "## ç²¾é€‰èµ„æºæ¨è\n\n"
    
    for item in selected_items:
        title = generate_title(item)
        emoji = random.choice(["ğŸš€", "âš¡", "ğŸŒ", "ğŸ”¥", "ğŸ’¡", "ğŸ“", "â­", "ğŸ’"])
        
        md_content += f"### {emoji} [{title}]({item['url']})\n\n"
        
        # ç”Ÿæˆç®€çŸ­æè¿°
        if item.get('type') == 'article':
            # å¯¹äºæ–‡ç« ï¼Œç”Ÿæˆç®€å•çš„é˜…è¯»å¼•å¯¼
            desc_templates = [
                "ç‚¹å‡»é˜…è¯»å…¨æ–‡ï¼Œäº†è§£æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚",
                "æœ€æ–°æ›´æ–°å†…å®¹ï¼Œä¸å®¹é”™è¿‡ã€‚",
                "å¹²è´§æ»¡æ»¡ï¼Œå»ºè®®æ”¶è—é˜…è¯»ã€‚",
                "æ·±åº¦è§£æï¼Œå¸¦ä½ äº†è§£æ›´å¤šã€‚",
                "ç‚¹å‡»ä¸Šæ–¹é“¾æ¥æŸ¥çœ‹å®Œæ•´æ•™ç¨‹/è¯„æµ‹ã€‚"
            ]
        else:
            desc_templates = [
                f"ç‚¹å‡»ä¸Šæ–¹é“¾æ¥è®¿é—® {item['name']} å®˜ç½‘ï¼Œè·å–æœ€æ–°ä¼˜æƒ ã€‚",
                f"{item['name']} æ˜¯ä¸€æ¬¾æ€§ä»·æ¯”æé«˜çš„åŠ é€ŸæœåŠ¡ï¼Œæ”¯æŒå¤šå¹³å°ä½¿ç”¨ã€‚",
                f"æ™šé«˜å³° 4K è§†é¢‘ç§’å¼€ï¼Œ{item['name']} å€¼å¾—ä¸€è¯•ã€‚",
                f"æ³¨å†Œå³å¯å…è´¹è¯•ç”¨ï¼Œ{item['name']} æä¾›ç¨³å®šé«˜é€Ÿçš„èŠ‚ç‚¹ã€‚",
                "ä¸“çº¿æ¥å…¥ï¼Œè¶…ä½å»¶è¿Ÿï¼Œæ¸¸æˆ/è§†é¢‘ä¸¤ä¸è¯¯ã€‚"
            ]
        md_content += f"{random.choice(desc_templates)}\n\n"
    
    md_content += "---\n"
    md_content += "### å…è´£å£°æ˜\n"
    md_content += "æœ¬æ–‡å†…å®¹ä»…ä¾›å­¦ä¹ å’ŒæŠ€æœ¯äº¤æµä½¿ç”¨ï¼Œè¯·å‹¿ç”¨äºéæ³•ç”¨é€”ã€‚è¯·éµå®ˆå½“åœ°æ³•å¾‹æ³•è§„ã€‚\n\n"
    md_content += f"*è‡ªåŠ¨æ›´æ–°äº {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"
    
    return md_content

def main():
    print("å¼€å§‹ç”Ÿæˆå†…å®¹...")
    
    # 1. è·å–æ‰€æœ‰ç«™ç‚¹
    configs = get_all_site_configs()
    if not configs:
        print("No site configs found.")
        return

    # 2. éšæœºé€‰æ‹©ä¸€ä¸ªç«™ç‚¹ (å°è¯•æœ€å¤š3æ¬¡ï¼Œä»¥é˜²é€‰ä¸­çš„ç«™ç‚¹æ²¡æœ‰æ•°æ®)
    selected_items = []
    
    # Shuffle to pick randomly
    random.shuffle(configs)
    
    for config in configs:
        print(f"Trying site: {config.get('name', config['id'])}")
        items = fetch_site_data(config)
        if items:
            selected_items = items
            print(f"Successfully fetched {len(items)} items from {config['id']}")
            break
        print(f"No items found for {config['id']}, trying next...")
    
    if not selected_items:
        print("Failed to fetch content from any site.")
        return

    # 3. ç”Ÿæˆå†…å®¹
    content = generate_content(selected_items, count=random.randint(10, 20))
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"å†…å®¹å·²ç”Ÿæˆè‡³ {OUTPUT_FILE}")

if __name__ == "__main__":
    main()