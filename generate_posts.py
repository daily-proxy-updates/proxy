import os
import json
import random
import datetime
import requests
import xml.etree.ElementTree as ET
from urllib.parse import urlparse

# é…ç½®
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
SITES_DIR = os.path.join(PROJECT_ROOT, 'sites')
OUTPUT_FILE = os.path.join(PROJECT_ROOT, 'README.md')

# å…³é”®è¯åº“
KEYWORDS = [
    "æœºåœºæ¨è", "ç§‘å­¦ä¸Šç½‘", "æ¢¯å­æ¨è", "ç¿»å¢™è½¯ä»¶", "VPNæ¨è", 
    "ClashèŠ‚ç‚¹", "Shadowsocks", "V2Ray", "Trojan", "é«˜é€ŸèŠ‚ç‚¹",
    "è§£é”Netflix", "4Kç§’å¼€", "ç¨³å®šæœºåœº", "ä¾¿å®œæœºåœº", "IPLCä¸“çº¿"
]

# æ ‡é¢˜æ¨¡æ¿
TITLE_TEMPLATES = [
    "{name} æœºåœºæ¨è - é«˜é€Ÿç¨³å®š 4K ç§’å¼€",
    "2024 æœ€ä½³æœºåœºæ¨èï¼š{name} è¯„æµ‹",
    "{name} æ€ä¹ˆæ ·ï¼Ÿæœ€æ–°ä½¿ç”¨ä½“éªŒæŠ¥å‘Š",
    "ç¨³å®šå¥½ç”¨çš„æ¢¯å­æ¨èï¼š{name}",
    "{name} - è§£é”æµåª’ä½“ï¼Œæ™šé«˜å³°ä¸å¡é¡¿",
    "ä¾¿å®œæœºåœºæ¨èï¼š{name} æ€§ä»·æ¯”ä¹‹é€‰",
    "{name} å®˜ç½‘åœ°å€ä¸æœ€æ–°ä¼˜æƒ ç ",
    "å®‰å“/iOS/Mac/Windows é€šç”¨æœºåœºæ¨èï¼š{name}"
]

def fetch_feed_posts(proxy_host):
    """Fetch posts from the site's RSS feed"""
    # Try different schemes if https fails, but default to https
    url = f"https://{proxy_host}/feed"
    print(f"Fetching feed from: {url}")
    try:
        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        if response.status_code == 200:
            # Parse XML
            try:
                root = ET.fromstring(response.content)
            except ET.ParseError:
                # Try with utf-8 decoding if direct parse fails
                root = ET.fromstring(response.content.decode('utf-8'))
                
            posts = []
            # Standard RSS 2.0: channel -> item
            # Also handle Atom if needed, but WordPress uses RSS 2.0 by default at /feed
            for item in root.findall('./channel/item'):
                title = item.find('title')
                link = item.find('link')
                
                if title is not None and link is not None:
                    title_text = title.text
                    link_text = link.text
                    
                    # Ensure link uses proxy_host
                    if link_text:
                        parsed = urlparse(link_text)
                        # Reconstruct url with proxy_host
                        new_link = link_text.replace(parsed.netloc, proxy_host)
                        
                        posts.append({
                            'name': title_text, # Use title as name
                            'url': new_link,
                            'type': 'article'
                        })
            print(f"Found {len(posts)} posts for {proxy_host}")
            return posts
        else:
            print(f"Failed to fetch feed for {proxy_host}: Status {response.status_code}")
    except Exception as e:
        print(f"Error fetching feed for {proxy_host}: {e}")
    return []

def load_sites_and_links():
    """è¯»å–æ‰€æœ‰ç«™ç‚¹é…ç½®å’Œå¯¹åº”çš„é“¾æ¥æ–‡ä»¶"""
    sites_data = []
    
    if not os.path.exists(SITES_DIR):
        print(f"Error: Sites directory {SITES_DIR} not found.")
        return []

    # è·å–æ‰€æœ‰ç«™ç‚¹é…ç½®æ–‡ä»¶
    site_files = [f for f in os.listdir(SITES_DIR) if f.endswith('.json') and not f.endswith('_links.json')]
    
    for site_file in site_files:
        site_id = site_file.replace('.json', '')
        site_path = os.path.join(SITES_DIR, site_file)
        links_path = os.path.join(SITES_DIR, f"{site_id}_links.json")
        
        try:
            with open(site_path, 'r', encoding='utf-8') as f:
                site_config = json.load(f)
            
            # æ”¶é›†è¯¥ç«™ç‚¹çš„é“¾æ¥
            valid_links = []
            
            # 1. å¦‚æœæœ‰ links æ–‡ä»¶ï¼Œè¯»å–æ¨èé“¾æ¥
            if os.path.exists(links_path):
                with open(links_path, 'r', encoding='utf-8') as f:
                    links = json.load(f)
                
                for kw, url in links.items():
                    if url.startswith('http'):
                        valid_links.append({
                            'name': kw, # åŸå§‹å…³é”®è¯
                            'url': url,
                            'type': 'referral'
                        })
            
            # 2. å°†ç«™ç‚¹æœ¬èº«ä¹Ÿä½œä¸ºä¸€ä¸ªæ¨èï¼ˆå¦‚æœæ˜¯é•œåƒç«™ï¼‰
            if site_config.get('proxyHost'):
                proxy_host = site_config['proxyHost']
                if isinstance(proxy_host, list):
                    proxy_host = proxy_host[0]
                
                # Fetch RSS feed posts
                feed_posts = fetch_feed_posts(proxy_host)
                if feed_posts:
                    valid_links.extend(feed_posts)
                
                # Also add the main site link
                valid_links.append({
                    'name': site_config.get('name', site_id),
                    'url': f"https://{proxy_host}",
                    'type': 'site'
                })

            if valid_links:
                sites_data.append({
                    'site_name': site_config.get('name', site_id),
                    'links': valid_links
                })
                
        except Exception as e:
            print(f"Error processing {site_id}: {e}")
        
    return sites_data

def generate_title(item):
    """æ ¹æ®é“¾æ¥ç±»å‹ç”Ÿæˆæ ‡é¢˜"""
    name = item['name']
    
    # å¦‚æœæ˜¯æ–‡ç« ç±»å‹çš„é“¾æ¥ï¼Œç›´æ¥ä½¿ç”¨æ–‡ç« æ ‡é¢˜
    if item.get('type') == 'article':
        return name
        
    # é¦–å­—æ¯å¤§å†™
    name = name.capitalize() if name else "Unknown"
    
    template = random.choice(TITLE_TEMPLATES)
    return template.format(name=name)

def generate_content(sites_data, count=15):
    """ç”Ÿæˆ Markdown å†…å®¹"""
    all_items = []
    
    # å±•å¹³æ‰€æœ‰é“¾æ¥
    for site in sites_data:
        for link in site['links']:
            all_items.append(link)
    
    if not all_items:
        return "No articles found."

    # éšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„æ–‡ç« 
    selected_items = random.sample(all_items, min(count, len(all_items)))
    
    # ç”Ÿæˆ Markdown
    today = datetime.date.today().strftime("%Y-%m-%d")
    md_content = f"# æœºåœºæ¨èä¸ç½‘ç»œåŠ é€ŸæŒ‡å— ({today})\n\n"
    
    md_content += "> æœ¬æ–‡æ¯æ—¥è‡ªåŠ¨æ›´æ–°ï¼Œæ•´ç†äº†æœ€æ–°çš„ç½‘ç»œåŠ é€Ÿèµ„æºã€æœºåœºæ¨èä¸ç§‘å­¦ä¸Šç½‘æŠ€å·§ï¼ŒåŠ©ä½ ç•…æ¸¸äº’è”ç½‘ã€‚\n\n"
    
    # éšæœºæ’å…¥ä¸€äº›å…³é”®è¯æ®µè½
    tags = random.sample(KEYWORDS, min(5, len(KEYWORDS)))
    md_content += f"**çƒ­é—¨æ ‡ç­¾**ï¼š{'ã€'.join(tags)}\n\n"
    
    md_content += "## ç²¾é€‰èµ„æºæ¨è\n\n"
    
    for item in selected_items:
        title = generate_title(item)
        emoji = random.choice(["ğŸš€", "âš¡", "ğŸŒ", "ğŸ”¥", "ğŸ’¡", "ğŸ“", "â­", "ğŸ’"])
        
        md_content += f"### {emoji} [{title}]({item['url']})\n\n"
        
        # ç”Ÿæˆç®€çŸ­æè¿°
        if item.get('type') == 'article':
            # å¯¹äºæ–‡ç« ï¼Œç”Ÿæˆç®€å•çš„é˜…è¯»å¼•å¯¼
            desc_templates = [
                "ç‚¹å‡»é˜…è¯»å…¨æ–‡ï¼Œäº†è§£æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚",
                "æœ€æ–°æ›´æ–°å†…å®¹ï¼Œä¸å®¹é”™è¿‡ã€‚",
                "å¹²è´§æ»¡æ»¡ï¼Œå»ºè®®æ”¶è—é˜…è¯»ã€‚",
                "æ·±åº¦è§£æï¼Œå¸¦ä½ äº†è§£æ›´å¤šã€‚",
                "ç‚¹å‡»ä¸Šæ–¹é“¾æ¥æŸ¥çœ‹å®Œæ•´æ•™ç¨‹/è¯„æµ‹ã€‚"
            ]
        else:
            desc_templates = [
                f"ç‚¹å‡»ä¸Šæ–¹é“¾æ¥è®¿é—® {item['name']} å®˜ç½‘ï¼Œè·å–æœ€æ–°ä¼˜æƒ ã€‚",
                f"{item['name']} æ˜¯ä¸€æ¬¾æ€§ä»·æ¯”æé«˜çš„åŠ é€ŸæœåŠ¡ï¼Œæ”¯æŒå¤šå¹³å°ä½¿ç”¨ã€‚",
                f"æ™šé«˜å³° 4K è§†é¢‘ç§’å¼€ï¼Œ{item['name']} å€¼å¾—ä¸€è¯•ã€‚",
                f"æ³¨å†Œå³å¯å…è´¹è¯•ç”¨ï¼Œ{item['name']} æä¾›ç¨³å®šé«˜é€Ÿçš„èŠ‚ç‚¹ã€‚",
                "ä¸“çº¿æ¥å…¥ï¼Œè¶…ä½å»¶è¿Ÿï¼Œæ¸¸æˆ/è§†é¢‘ä¸¤ä¸è¯¯ã€‚"
            ]
        md_content += f"{random.choice(desc_templates)}\n\n"
    
    md_content += "---\n"
    md_content += "### å…è´£å£°æ˜\n"
    md_content += "æœ¬æ–‡å†…å®¹ä»…ä¾›å­¦ä¹ å’ŒæŠ€æœ¯äº¤æµä½¿ç”¨ï¼Œè¯·å‹¿ç”¨äºéæ³•ç”¨é€”ã€‚è¯·éµå®ˆå½“åœ°æ³•å¾‹æ³•è§„ã€‚\n\n"
    md_content += f"*è‡ªåŠ¨æ›´æ–°äº {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"
    
    return md_content

def main():
    print("å¼€å§‹ç”Ÿæˆå†…å®¹...")
    sites = load_sites_and_links()
    content = generate_content(sites, count=random.randint(10, 20))
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"å†…å®¹å·²ç”Ÿæˆè‡³ {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
