import os
import json
import random
import datetime
import requests
import re
import xml.etree.ElementTree as ET
from urllib.parse import urlparse, quote
from email.utils import parsedate_to_datetime

# é…ç½®
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
SITES_DIR = os.path.join(PROJECT_ROOT, 'sites')
OUTPUT_FILE = os.path.join(PROJECT_ROOT, 'README.md')
ARCHIVES_DIR = os.path.join(PROJECT_ROOT, 'archives')

# å…³é”®è¯åº“
KEYWORDS = [
    "æœºåœºæ¨è", "ç§‘å­¦ä¸Šç½‘", "æ¢¯å­æ¨è", "ç¿»å¢™è½¯ä»¶", "VPNæ¨è", 
    "ClashèŠ‚ç‚¹", "Shadowsocks", "V2Ray", "Trojan", "é«˜é€ŸèŠ‚ç‚¹",
    "è§£é”Netflix", "4Kç§’å¼€", "ç¨³å®šæœºåœº", "ä¾¿å®œæœºåœº", "IPLCä¸“çº¿"
]

# æ ‡é¢˜æ¨¡æ¿
TITLE_TEMPLATES = [
    "{name} æœºåœºæ¨è - é«˜é€Ÿç¨³å®š 4K ç§’å¼€",
    "2024 æœ€ä½³æœºåœºæ¨èï¼š{name} è¯„æµ‹",
    "{name} æ€ä¹ˆæ ·ï¼Ÿæœ€æ–°ä½¿ç”¨ä½“éªŒæŠ¥å‘Š",
    "ç¨³å®šå¥½ç”¨çš„æ¢¯å­æ¨èï¼š{name}",
    "{name} - è§£é”æµåª’ä½“ï¼Œæ™šé«˜å³°ä¸å¡é¡¿",
    "ä¾¿å®œæœºåœºæ¨èï¼š{name} æ€§ä»·æ¯”ä¹‹é€‰",
    "{name} å®˜ç½‘åœ°å€ä¸æœ€æ–°ä¼˜æƒ ç ",
    "å®‰å“/iOS/Mac/Windows é€šç”¨æœºåœºæ¨èï¼š{name}"
]

def sanitize_filename(name):
    """Sanitize filename to be safe for filesystem"""
    # Remove invalid characters
    name = re.sub(r'[\\/*?:"<>|]', "", name)
    # Replace spaces with hyphens
    name = name.replace(" ", "-")
    # Limit length
    return name[:100]

def fetch_feed_posts(proxy_host):
    """Fetch posts from the site's RSS feed"""
    # Try different schemes if https fails, but default to https
    url = f"https://{proxy_host}/feed"
    print(f"Fetching feed from: {url}")
    try:
        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        if response.status_code == 200:
            # Parse XML
            try:
                content = response.content
                try:
                    text = content.decode('utf-8')
                except UnicodeDecodeError:
                    text = content.decode('gbk', errors='ignore')
                root = ET.fromstring(text)
            except ET.ParseError as e:
                print(f"XML Parse Error for {proxy_host}: {e}")
                return []
                
            posts = []
            # Standard RSS 2.0: channel -> item
            for item in root.findall('./channel/item'):
                title = item.find('title')
                link = item.find('link')
                description = item.find('description')
                pub_date = item.find('pubDate')
                
                if title is not None and link is not None:
                    title_text = title.text
                    link_text = link.text
                    desc_text = description.text if description is not None else ""
                    date_text = pub_date.text if pub_date is not None else ""
                    
                    # Parse date
                    try:
                        if date_text:
                            dt = parsedate_to_datetime(date_text)
                            # Convert to naive datetime for consistency if needed, but keeping tz info is fine
                            # For file system, we might want local time or UTC. Let's stick to UTC or input time.
                        else:
                            dt = datetime.datetime.now()
                    except Exception as e:
                        print(f"Date parse error: {e}")
                        dt = datetime.datetime.now()

                    # Ensure link uses proxy_host
                    if link_text:
                        parsed = urlparse(link_text)
                        # Reconstruct url with proxy_host
                        new_link = link_text.replace(parsed.netloc, proxy_host)
                        
                        posts.append({
                            'name': title_text, # Use title as name
                            'url': new_link,
                            'type': 'article',
                            'description': desc_text,
                            'date': dt
                        })
            print(f"Found {len(posts)} posts for {proxy_host}")
            return posts
        else:
            print(f"Failed to fetch feed for {proxy_host}: Status {response.status_code}")
    except Exception as e:
        print(f"Error fetching feed for {proxy_host}: {e}")
    return []

def save_article_to_archive(item, proxy_host):
    """Save article to local markdown file in archives/YYYY/MM"""
    dt = item['date']
    # If dt is struct_time or similar, ensure it's datetime
    if not isinstance(dt, (datetime.datetime, datetime.date)):
         dt = datetime.datetime.now()
         
    year = dt.strftime("%Y")
    month = dt.strftime("%m")
    day = dt.strftime("%d")
    
    # Create dir: archives/YYYY/MM
    archive_dir = os.path.join(ARCHIVES_DIR, year, month)
    os.makedirs(archive_dir, exist_ok=True)
    
    # Filename: DD-title.md
    safe_title = sanitize_filename(item['name'])
    filename = f"{day}-{safe_title}.md"
    filepath = os.path.join(archive_dir, filename)
    
    # Relative path for README (URL encoded for markdown link)
    # On Windows this might need separator handling, but for GitHub/Linux it's forward slash
    rel_path = f"archives/{year}/{month}/{quote(filename)}"
    
    # Content generation
    content = f"# {item['name']}\n\n"
    content += f"**å‘å¸ƒæ—¶é—´**: {dt.strftime('%Y-%m-%d %H:%M')}\n\n"
    
    if item.get('description'):
        # Clean up description (simple tag stripping if needed, but raw is okay for now)
        desc = item['description']
        content += f"## æ‘˜è¦\n\n{desc}\n\n"
    
    content += f"---\n\n"
    content += f"## é˜…è¯»å…¨æ–‡\n\n"
    content += f"[ç‚¹å‡»æ­¤å¤„é˜…è¯»å®Œæ•´æ–‡ç« ]({item['url']})\n\n"
    content += f"---\n"
    content += f"*æœ¬æ–‡ç« ç”± [Mirror Proxy](https://github.com/daily-proxy-updates/proxy) è‡ªåŠ¨å½’æ¡£ï¼ŒåŸå§‹å†…å®¹æ¥è‡ª [{proxy_host}](https://{proxy_host})*\n"
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)
        
    return rel_path

def get_all_site_configs():
    """è·å–æ‰€æœ‰ç«™ç‚¹é…ç½®"""
    configs = []
    if not os.path.exists(SITES_DIR):
        print(f"Error: Sites directory {SITES_DIR} not found.")
        return []

    site_files = [f for f in os.listdir(SITES_DIR) if f.endswith('.json') and not f.endswith('_links.json')]
    
    for site_file in site_files:
        try:
            site_path = os.path.join(SITES_DIR, site_file)
            with open(site_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                # Ensure we have the filename id
                config['id'] = site_file.replace('.json', '')
                configs.append(config)
        except Exception as e:
            print(f"Error loading config {site_file}: {e}")
            
    return configs

def fetch_site_data(site_config):
    """è·å–å•ä¸ªç«™ç‚¹çš„æ•°æ®ï¼ˆæ–‡ç« å’Œé“¾æ¥ï¼‰"""
    site_id = site_config.get('id')
    proxy_host = site_config.get('proxyHost')
    
    if not proxy_host:
        return []
        
    if isinstance(proxy_host, list):
        proxy_host = proxy_host[0]
        
    valid_links = []
    
    # 1. è·å– RSS æ–‡ç«  (Sub-pages)
    feed_posts = fetch_feed_posts(proxy_host)
    
    # Save posts to archive and update item with local path
    for post in feed_posts:
        local_path = save_article_to_archive(post, proxy_host)
        post['local_path'] = local_path # Store local path for README
        valid_links.append(post)
        
    # 2. è¯»å–å¹¶è½¬æ¢æ¨å¹¿é“¾æ¥ (Redirects)
    links_path = os.path.join(SITES_DIR, f"{site_id}_links.json")
    if os.path.exists(links_path):
        try:
            with open(links_path, 'r', encoding='utf-8') as f:
                links = json.load(f)
            
            for kw, url in links.items():
                if url.startswith('http'):
                    encoded_kw = quote(kw)
                    local_url = f"https://{proxy_host}/{encoded_kw}"
                    
                    valid_links.append({
                        'name': kw,
                        'url': local_url,
                        'type': 'referral'
                    })
        except Exception as e:
            print(f"Error loading links for {site_id}: {e}")

    # 3. æ·»åŠ ç«™ç‚¹ä¸»é¡µ
    valid_links.append({
        'name': site_config.get('name', site_id),
        'url': f"https://{proxy_host}",
        'type': 'site'
    })
    
    return valid_links

def generate_title(item):
    """æ ¹æ®é“¾æ¥ç±»å‹ç”Ÿæˆæ ‡é¢˜"""
    name = item['name']
    
    # å¦‚æœæ˜¯æ–‡ç« ç±»å‹çš„é“¾æ¥ï¼Œç›´æ¥ä½¿ç”¨æ–‡ç« æ ‡é¢˜
    if item.get('type') == 'article':
        return name
        
    # é¦–å­—æ¯å¤§å†™
    name = name.capitalize() if name else "Unknown"
    
    template = random.choice(TITLE_TEMPLATES)
    return template.format(name=name)

def generate_content(all_items, count=15):
    """ç”Ÿæˆ Markdown å†…å®¹"""
    if not all_items:
        return "No articles found."

    # éšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„æ–‡ç« 
    selected_items = random.sample(all_items, min(count, len(all_items)))
    
    # ç”Ÿæˆ Markdown
    today = datetime.date.today().strftime("%Y-%m-%d")
    md_content = f"# æ¯æ—¥ç§‘æŠ€èµ„è®¯ä¸ç½‘ç»œåŠ é€Ÿå½’æ¡£ ({today})\n\n"
    
    md_content += "> æœ¬é¡¹ç›®è‡ªåŠ¨æŠ“å–å¹¶å½’æ¡£æœ€æ–°çš„ç§‘æŠ€èµ„è®¯ã€ç½‘ç»œåŠ é€Ÿèµ„æºä¸æ•™ç¨‹ã€‚æ‰€æœ‰å†…å®¹å‡å·²æ°¸ä¹…ä¿å­˜è‡³ GitHubã€‚\n\n"
    
    # éšæœºæ’å…¥ä¸€äº›å…³é”®è¯æ®µè½
    tags = random.sample(KEYWORDS, min(5, len(KEYWORDS)))
    md_content += f"**çƒ­é—¨æ ‡ç­¾**ï¼š{'ã€'.join(tags)}\n\n"
    
    md_content += "## æœ€æ–°å½’æ¡£ (Latest Archives)\n\n"
    
    # Split items into articles and referrals
    articles = [i for i in selected_items if i.get('type') == 'article']
    referrals = [i for i in selected_items if i.get('type') != 'article']
    
    # List articles first (high quality content)
    if articles:
        for item in articles:
            title = generate_title(item)
            # Link to LOCAL archive file if available, otherwise direct link
            link_url = item.get('local_path', item['url'])
            
            md_content += f"### ğŸ“„ [{title}]({link_url})\n"
            # Add a small snippet if available
            if item.get('description'):
                # Take first 100 chars
                snippet = re.sub(r'<[^>]+>', '', item['description'])[:100] + "..."
                md_content += f"> {snippet}\n\n"
            else:
                md_content += f"> ç‚¹å‡»æŸ¥çœ‹å½’æ¡£å…¨æ–‡...\n\n"

    md_content += "## æ¨èèµ„æº (Recommended Resources)\n\n"
    
    for item in referrals:
        title = generate_title(item)
        emoji = random.choice(["ğŸš€", "âš¡", "ğŸŒ", "ğŸ”¥", "ğŸ’¡", "ğŸ“", "â­", "ğŸ’"])
        
        md_content += f"### {emoji} [{title}]({item['url']})\n"
        
        desc_templates = [
            f"ç‚¹å‡»ä¸Šæ–¹é“¾æ¥è®¿é—® {item['name']} å®˜ç½‘ï¼Œè·å–æœ€æ–°ä¼˜æƒ ã€‚",
            f"{item['name']} æ˜¯ä¸€æ¬¾æ€§ä»·æ¯”æé«˜çš„åŠ é€ŸæœåŠ¡ï¼Œæ”¯æŒå¤šå¹³å°ä½¿ç”¨ã€‚",
            f"æ™šé«˜å³° 4K è§†é¢‘ç§’å¼€ï¼Œ{item['name']} å€¼å¾—ä¸€è¯•ã€‚",
            f"æ³¨å†Œå³å¯å…è´¹è¯•ç”¨ï¼Œ{item['name']} æä¾›ç¨³å®šé«˜é€Ÿçš„èŠ‚ç‚¹ã€‚",
            "ä¸“çº¿æ¥å…¥ï¼Œè¶…ä½å»¶è¿Ÿï¼Œæ¸¸æˆ/è§†é¢‘ä¸¤ä¸è¯¯ã€‚"
        ]
        md_content += f"{random.choice(desc_templates)}\n\n"
    
    md_content += "---\n"
    md_content += "### å…è´£å£°æ˜\n"
    md_content += "æœ¬æ–‡å†…å®¹ä»…ä¾›å­¦ä¹ å’ŒæŠ€æœ¯äº¤æµä½¿ç”¨ï¼Œè¯·å‹¿ç”¨äºéæ³•ç”¨é€”ã€‚è¯·éµå®ˆå½“åœ°æ³•å¾‹æ³•è§„ã€‚\n\n"
    md_content += f"*è‡ªåŠ¨æ›´æ–°äº {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"
    
    return md_content

def main():
    print("å¼€å§‹ç”Ÿæˆå†…å®¹...")
    
    # 1. è·å–æ‰€æœ‰ç«™ç‚¹
    configs = get_all_site_configs()
    if not configs:
        print("No site configs found.")
        return

    # 2. éšæœºé€‰æ‹©ä¸€ä¸ªç«™ç‚¹
    selected_items = []
    
    random.shuffle(configs)
    
    for config in configs:
        print(f"Trying site: {config.get('name', config['id'])}")
        items = fetch_site_data(config)
        if items:
            selected_items = items
            print(f"Successfully fetched {len(items)} items from {config['id']}")
            break
        print(f"No items found for {config['id']}, trying next...")
    
    if not selected_items:
        print("Failed to fetch content from any site.")
        return

    # 3. ç”Ÿæˆå†…å®¹
    content = generate_content(selected_items, count=20)
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"å†…å®¹å·²ç”Ÿæˆè‡³ {OUTPUT_FILE}")

if __name__ == "__main__":
    main()